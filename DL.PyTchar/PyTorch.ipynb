{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchvision in /home/alexrf14/.local/lib/python3.12/site-packages (0.20.1+rocm6.2)\n",
      "Requirement already satisfied: torchaudio in /home/alexrf14/.local/lib/python3.12/site-packages (2.5.1+rocm6.2)\n",
      "Requirement already satisfied: filelock in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/alexrf14/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/alexrf14/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/alexrf14/.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/alexrf14/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/alexrf14/.local/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Installing collected packages: pillow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pillow-11.1.0 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/alexrf14/.local/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/alexrf14/.local/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/alexrf14/.local/lib/python3.12/site-packages (from scikit-learn) (1.15.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib\n",
      "Successfully installed joblib-1.4.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/alexrf14/.local/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.55.5-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/alexrf14/.local/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/alexrf14/anaconda3/envs/pyTorch-gpu/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.55.5-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.5 kiwisolver-1.4.8 matplotlib-3.10.0 pyparsing-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Force PyTorch to use CUDA even if ROCm is available\n",
    "torch.cuda.set_device(0)  # Assuming device 0 is your CUDA-enabled device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba de PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n",
      "Mean Squared Error (MSE): 134.5720751569695\n",
      "Median Absolute Error (MedAE): 3.9538956079101553\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dataframe\n",
    "test_data = pd.read_csv(\"public_test.csv\")\n",
    "train_data = pd.read_csv(\"public_train.csv\")\n",
    "\n",
    "# Identificar columnas\n",
    "TARGET = \"ccs\"\n",
    "CATEGORICAL_FEATURES = [\"adduct\"]\n",
    "NUMERIC_FEATURES = [col for col in train_data.columns if col.startswith(\"desc_\")]\n",
    "BINARY_FEATURES = [col for col in train_data.columns if col.startswith(\"fgp_\")]\n",
    "\n",
    "# Separar características y objetivo\n",
    "x = train_data.drop(columns=[TARGET])  # Características\n",
    "y = train_data[TARGET]  # Valor a estimar\n",
    "\n",
    "# Dividir en datos de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "class SimpleNN(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_dim, hidden_dim=64, lr=0.001, epochs=100):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if(torch.device == \"cpu\"):\n",
    "            print(\"cpu\")\n",
    "        else:\n",
    "            print(\"gpu\")\n",
    "\n",
    "        # Red neuronal\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, 1)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X y y son ahora ndarray\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(X_tensor)\n",
    "            loss = self.criterion(predictions, y_tensor)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor)\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "\n",
    "# Crear el modelo con los parámetros deseados\n",
    "model_params = {'hidden_dim': 64, 'lr': 0.001, 'epochs': 40}\n",
    "\n",
    "# Crear el pipeline de preprocesamiento y modelo\n",
    "def create_preproc_and_model(numeric_features, categorical_features, binary_features):\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # Imputar valores faltantes y luego estandarizar los datos numéricos\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')),  # Usar la media para imputar\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numeric_features),\n",
    "\n",
    "            # Imputar valores faltantes en los datos categóricos y aplicar OneHotEncoder\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),  # Usar la moda para imputar\n",
    "                ('onehot', OneHotEncoder())\n",
    "            ]), categorical_features),\n",
    "\n",
    "            # Imputar valores faltantes y luego estandarizar los datos binarios\n",
    "            ('bin', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),  # Usar la moda para imputar\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), binary_features)\n",
    "        ])\n",
    "    \n",
    "    # Ajusta el preprocesador a los datos de entrenamiento\n",
    "    preprocessor.fit(x_train)\n",
    "    \n",
    "    # Verificar la forma de los datos después del preprocesamiento\n",
    "    x_train_transformed = preprocessor.transform(x_train)\n",
    "    \n",
    "    input_dim = x_train_transformed.shape[1]  # Número de características después del preprocesamiento\n",
    "    model = SimpleNN(input_dim=input_dim, hidden_dim=128, lr=0.00001, epochs=50000)\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "pipeline = create_preproc_and_model(NUMERIC_FEATURES, CATEGORICAL_FEATURES, BINARY_FEATURES)\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_data = pd.read_csv(\"public_test.csv\")\n",
    "train_data = pd.read_csv(\"public_train.csv\")\n",
    "\n",
    "# Identificar columnas\n",
    "TARGET = \"ccs\"\n",
    "CATEGORICAL_FEATURES = [\"adduct\"]\n",
    "NUMERIC_FEATURES = [col for col in train_data.columns if col.startswith(\"desc_\")]\n",
    "BINARY_FEATURES = [col for col in train_data.columns if col.startswith(\"fgp_\")]\n",
    "\n",
    "# Separar características y objetivo\n",
    "x = train_data.drop(columns=[TARGET])  # Características\n",
    "y = train_data[TARGET]\n",
    "\n",
    "class SimpleNN(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_dim, hidden_dim=64, lr=0.001, epochs=100):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Red neuronal\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, 1)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(X_tensor)\n",
    "            loss = self.criterion(predictions, y_tensor)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Guardar el MSE de cada época\n",
    "            self.loss_history.append(loss.item())\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor)\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "# Crear el pipeline de preprocesamiento y modelo\n",
    "def create_preproc_and_model(numeric_features, categorical_features, binary_features):\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder())\n",
    "            ]), categorical_features),\n",
    "            ('bin', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), binary_features)\n",
    "        ])\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    preprocessor.fit(x_train)\n",
    "    x_train_transformed = preprocessor.transform(x_train)\n",
    "    input_dim = x_train_transformed.shape[1]\n",
    "    model = SimpleNN(input_dim=input_dim, hidden_dim=1028, lr=0.00002, epochs=10000)\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline, x_train, x_test, y_train, y_test\n",
    "\n",
    "pipeline, x_train, x_test, y_train, y_test = create_preproc_and_model(NUMERIC_FEATURES, CATEGORICAL_FEATURES, BINARY_FEATURES)\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Graficar la evolución del MSE\n",
    "model = pipeline.named_steps['model']  # Obtener el modelo entrenado\n",
    "plt.plot(model.loss_history)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Evolución del MSE durante el entrenamiento')\n",
    "plt.show()\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "# Calcular y mostrar MSE y MedAE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R² Score:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos más capas + Regulaciones + Drop Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, RegressorMixin\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompose\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnTransformer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/__init__.py:87\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     84\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     90\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    134\u001b[0m     ]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/__init__.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/_joblib.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m     _warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# joblib imports may raise DeprecationWarning on certain Python\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# versions\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m         Memory,\n\u001b[1;32m     10\u001b[0m         Parallel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         register_parallel_backend,\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     24\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_parallel_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"public_test.csv\")\n",
    "train_data = pd.read_csv(\"public_train.csv\")\n",
    "\n",
    "# Identificar columnas\n",
    "TARGET = \"ccs\"\n",
    "CATEGORICAL_FEATURES = [\"adduct\"]\n",
    "NUMERIC_FEATURES = [col for col in train_data.columns if col.startswith(\"desc_\")]\n",
    "BINARY_FEATURES = [col for col in train_data.columns if col.startswith(\"fgp_\")]\n",
    "\n",
    "# Separar características y objetivo\n",
    "x = train_data.drop(columns=[TARGET])  # Características\n",
    "y = train_data[TARGET]\n",
    "\n",
    "class SimpleNN(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32, 16], lr=0.001, epochs=100, batch_size=32, activation='ReLU', dropout=0.0):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Definir las capas de la red neuronal\n",
    "        layers = []\n",
    "        prev_dim = self.input_dim\n",
    "\n",
    "        for hidden_dim in self.hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            if self.activation == 'ReLU':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif self.activation == 'Tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif self.activation == 'Sigmoid':\n",
    "                layers.append(nn.Sigmoid())\n",
    "            elif self.activation == 'LeakyReLU':\n",
    "                layers.append(nn.LeakyReLU())\n",
    "            if self.dropout > 0:\n",
    "                layers.append(nn.Dropout(self.dropout))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        # Capa de salida\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "\n",
    "        # Crear el modelo secuencial\n",
    "        self.model = nn.Sequential(*layers).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.model(X_batch)\n",
    "                loss = self.criterion(predictions, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            self.loss_history.append(epoch_loss / len(dataloader))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor)\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "# Crear el pipeline de preprocesamiento y modelo\n",
    "# Crear el pipeline de preprocesamiento y modelo\n",
    "def create_preproc_and_model(numeric_features, categorical_features, binary_features):\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder())\n",
    "            ]), categorical_features),\n",
    "            ('bin', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), binary_features)\n",
    "        ])\n",
    "    \n",
    "    # Dividir los datos en entrenamiento y prueba\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Asegúrate de ajustar el preprocesador antes de cualquier transformación\n",
    "    preprocessor.fit(x_train)\n",
    "\n",
    "    # Transformar los datos de entrenamiento\n",
    "    x_train_transformed = preprocessor.transform(x_train)\n",
    "    input_dim = x_train_transformed.shape[1]\n",
    "\n",
    "    # Crear el modelo\n",
    "    model = SimpleNN(input_dim=input_dim ,lr = 0.0009, hidden_dims= [64, 32, 16], epochs = 500, dropout = 0.2, batch_size = 64, activation = 'ReLU')\n",
    "\n",
    "    # Crear el pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline, x_train, x_test, y_train, y_test\n",
    "\n",
    "# Crear el pipeline\n",
    "pipeline, x_train, x_test, y_train, y_test = create_preproc_and_model(NUMERIC_FEATURES, CATEGORICAL_FEATURES, BINARY_FEATURES)\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Graficar la evolución del MSE\n",
    "model = pipeline.named_steps['model']  # Obtener el modelo entrenado\n",
    "plt.plot(model.loss_history)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Evolución del MSE durante el entrenamiento')\n",
    "plt.show()\n",
    "\n",
    "# Hacer predicciones\n",
    "y_pred = pipeline.predict(x_test)\n",
    "\n",
    "# Calcular y mostrar MSE y MedAE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R² Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error, r2_score, make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leer los datos\n",
    "test_data = pd.read_csv(\"public_test.csv\")\n",
    "train_data = pd.read_csv(\"public_train.csv\")\n",
    "\n",
    "# Identificar columnas\n",
    "TARGET = \"ccs\"\n",
    "CATEGORICAL_FEATURES = [\"adduct\"]\n",
    "NUMERIC_FEATURES = [col for col in train_data.columns if col.startswith(\"desc_\")]\n",
    "BINARY_FEATURES = [col for col in train_data.columns if col.startswith(\"fgp_\")]\n",
    "\n",
    "# Separar características y objetivo\n",
    "x = train_data.drop(columns=[TARGET])  # Características\n",
    "y = train_data[TARGET]\n",
    "\n",
    "# Definir el modelo\n",
    "class SimpleNN(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32, 16], lr=0.001, epochs=100, batch_size=32, activation='ReLU', dropout=0.0):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.device.type == \"cpu\":\n",
    "            print(\"Using CPU\")\n",
    "        else:\n",
    "            print(\"Using GPU\")\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = self.input_dim\n",
    "        for hidden_dim in self.hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(getattr(nn, self.activation)())\n",
    "            if self.dropout > 0:\n",
    "                layers.append(nn.Dropout(self.dropout))\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.model = nn.Sequential(*layers).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.model(X_batch)\n",
    "                loss = self.criterion(predictions, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            self.loss_history.append(epoch_loss / len(dataloader))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(X_tensor)\n",
    "        return predictions.cpu().numpy()\n",
    "\n",
    "# Crear el pipeline de preprocesamiento y modelo\n",
    "def create_preproc_and_model(numeric_features, categorical_features, binary_features):\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('onehot', OneHotEncoder())\n",
    "            ]), categorical_features),\n",
    "            ('bin', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ]), binary_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    preprocessor.fit(x_train)\n",
    "    x_train_transformed = preprocessor.transform(x_train)\n",
    "    input_dim = x_train_transformed.shape[1]\n",
    "\n",
    "    # Convertir x_train y x_test a DataFrames para que ColumnTransformer pueda funcionar\n",
    "    x_train = pd.DataFrame(x_train, columns=train_data.drop(columns=[TARGET]).columns)\n",
    "    x_test = pd.DataFrame(x_test, columns=train_data.drop(columns=[TARGET]).columns)\n",
    "\n",
    "    model = SimpleNN(input_dim=input_dim, hidden_dims=[128, 64, 32], lr=0.0001, epochs=500, batch_size=64, activation='LeakyReLU', dropout=0.3)\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline, x_train, x_test, y_train, y_test\n",
    "\n",
    "# Crear y configurar el RandomizedSearchCV\n",
    "pipeline, x_train, x_test, y_train, y_test = create_preproc_and_model(NUMERIC_FEATURES, CATEGORICAL_FEATURES, BINARY_FEATURES)\n",
    "\n",
    "param_dist = {\n",
    "    'model__hidden_dims': [[128, 64, 32], [64, 32, 16], [256, 128, 64]],\n",
    "    'model__lr': [0.01, 0.001, 0.0001, 0.00001],\n",
    "    'model__epochs': [100, 500, 1000],\n",
    "    'model__batch_size': [32, 64, 128],\n",
    "    'model__activation': ['ReLU', 'LeakyReLU'],\n",
    "    'model__dropout': [0.0, 0.2, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "medae_scorer = make_scorer(median_absolute_error, greater_is_better=False)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring=medae_scorer\n",
    ")\n",
    "\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"Mejores parámetros encontrados:\", random_search.best_params_)\n",
    "\n",
    "y_pred = random_search.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R² Score:\", r2)\n",
    "\n",
    "# Graficar la evolución del MSE\n",
    "best_model = random_search.best_estimator_.named_steps['model']\n",
    "plt.plot(best_model.loss_history)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Evolución del MSE durante el entrenamiento')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigar torch.profiler y DataLoader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
